{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018d087c",
   "metadata": {},
   "source": [
    "# Text Preprocessing in NLP\n",
    "\n",
    "This notebook covers the fundamental text preprocessing techniques essential for Natural Language Processing. We'll explore various preprocessing methods and build a comprehensive pipeline for different NLP tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the importance of text preprocessing in NLP\n",
    "- Learn tokenization, normalization, and text cleaning techniques\n",
    "- Master stemming and lemmatization\n",
    "- Handle stop words and special characters\n",
    "- Build reusable preprocessing pipelines\n",
    "- Compare different preprocessing approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8ce6b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T04:34:36.789500Z",
     "start_time": "2025-10-07T04:34:36.503899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# NLP Libraries\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Load spaCy model (need to install: python -m spacy download en_core_web_sm)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except IOError:\n",
    "    print(\"spaCy English model not found. Please install with: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefadce7",
   "metadata": {},
   "source": [
    "## 1. What is Text Preprocessing?\n",
    "\n",
    "Text preprocessing is the crucial first step in any NLP pipeline. Raw text is messy and inconsistent - it contains:\n",
    "- **Punctuation and special characters**: \"Hello, World!\"\n",
    "- **Different cases**: \"Hello\" vs \"hello\"\n",
    "- **Extra whitespace**: \"Hello    World\"\n",
    "- **Common words**: \"the\", \"is\", \"and\" (often not meaningful)\n",
    "- **Different word forms**: \"running\", \"runs\", \"ran\" (same concept)\n",
    "\n",
    "**Goal**: Clean and standardize text so algorithms can process it effectively.\n",
    "\n",
    "### Why is it important?\n",
    "- **Consistency**: Ensures \"Hello\" and \"hello\" are treated the same\n",
    "- **Efficiency**: Removes noise and focuses on meaningful words\n",
    "- **Performance**: Better preprocessing often leads to better model results\n",
    "\n",
    "Let's explore each preprocessing technique step by step! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4d4a66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T04:42:34.675856Z",
     "start_time": "2025-10-07T04:42:34.669857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Original Text:\n",
      "'\\n    Hello, World! This is an EXAMPLE text with:\\n    - Different CASES (Upper, lower, MiXeD)\\n    - Punctuation marks!!! And special characters @#$%\\n    - Extra    spaces   and    tabs\\n    - Numbers like 123 and 456\\n    - URLs like https://example.com\\n    - Email addresses like user@example.com\\n    - Common words: the, is, and, of, to, in\\n    - Different word forms: running, runs, ran, runner\\n'\n",
      "\n",
      "==================================================\n",
      "ðŸ” Text Visualization:\n",
      "\n",
      "    Hello, World! This is an EXAMPLE text with:\n",
      "    - Different CASES (Upper, lower, MiXeD)\n",
      "    - Punctuation marks!!! And special characters @#$%\n",
      "    - Extra    spaces   and    tabs\n",
      "    - Numbers like 123 and 456\n",
      "    - URLs like https://example.com\n",
      "    - Email addresses like user@example.com\n",
      "    - Common words: the, is, and, of, to, in\n",
      "    - Different word forms: running, runs, ran, runner\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "    Hello, World! This is an EXAMPLE text with:\n",
    "    - Different CASES (Upper, lower, MiXeD)\n",
    "    - Punctuation marks!!! And special characters @#$%\n",
    "    - Extra    spaces   and    tabs\n",
    "    - Numbers like 123 and 456\n",
    "    - URLs like https://example.com\n",
    "    - Email addresses like user@example.com\n",
    "    - Common words: the, is, and, of, to, in\n",
    "    - Different word forms: running, runs, ran, runner\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“ Original Text:\")\n",
    "print(repr(sample_text))\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ” Text Visualization:\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e19f7f",
   "metadata": {},
   "source": [
    "## 2. Tokenization - Breaking Text into Pieces\n",
    "\n",
    "**Tokenization** is the process of splitting text into individual units called tokens (usually words or sentences).\n",
    "\n",
    "### Types of Tokenization:\n",
    "1. **Word Tokenization**: Split into words\n",
    "   - `\"I love AI\"` â†’ `[\"I\", \"love\", \"AI\"]`\n",
    "2. **Sentence Tokenization**: Split into sentences  \n",
    "   - `\"Hello. How are you?\"` â†’ `[\"Hello.\", \"How are you?\"]`\n",
    "3. **Subword Tokenization**: Split into smaller units (advanced)\n",
    "\n",
    "### Why Tokenization Matters:\n",
    "- **Foundation**: All other preprocessing steps work on tokens\n",
    "- **Challenges**: Handling punctuation, contractions, special cases\n",
    "- **Example**: `\"Don't\"` â†’ `[\"Don\", \"'t\"]` or `[\"Don't\"]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f274df35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T05:10:46.318629Z",
     "start_time": "2025-10-07T05:10:46.173186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Simple Split by Spaces:\n",
      "Tokens: ['Hello,', 'World!', 'This', 'is', 'an', 'EXAMPLE', 'text', 'with:', '-', 'Different']...\n",
      "Total tokens: 58\n",
      "\n",
      "ðŸ”¹ NLTK Word Tokenization:\n",
      "Tokens: ['Hello', ',', 'World', '!', 'This', 'is', 'an', 'EXAMPLE', 'text', 'with', ':', '-', 'Different', 'CASES', '(']...\n",
      "Total tokens: 85\n",
      "\n",
      "ðŸ”¹ Sentence Tokenization:\n",
      "Sentence 1: Hello, World!\n",
      "Sentence 2: This is an EXAMPLE text with:\n",
      "    - Different CASES (Upper, lower, MiXeD)\n",
      "    - Punctuation marks!!!\n",
      "Sentence 3: And special characters @#$%\n",
      "    - Extra    spaces   and    tabs\n",
      "    - Numbers like 123 and 456\n",
      "    - URLs like https://example.com\n",
      "    - Email addresses like user@example.com\n",
      "    - Common words: the, is, and, of, to, in\n",
      "    - Different word forms: running, runs, ran, runner\n",
      "\n",
      "ðŸ”¹ spaCy Tokenization:\n",
      "Tokens: ['\\n    ', 'Hello', ',', 'World', '!', 'This', 'is', 'an', 'EXAMPLE', 'text', 'with', ':', '\\n    ', '-', 'Different']...\n",
      "Total tokens: 91\n",
      "\n",
      "ðŸ” Token Details (first 10):\n",
      "'\n",
      "    ' -> POS: SPACE, Lemma: \n",
      "    \n",
      "'Hello' -> POS: PROPN, Lemma: Hello\n",
      "',' -> POS: PUNCT, Lemma: ,\n",
      "'World' -> POS: PROPN, Lemma: World\n",
      "'!' -> POS: PUNCT, Lemma: !\n",
      "'This' -> POS: PRON, Lemma: this\n",
      "'is' -> POS: AUX, Lemma: be\n",
      "'an' -> POS: DET, Lemma: an\n",
      "'EXAMPLE' -> POS: NOUN, Lemma: example\n",
      "'text' -> POS: NOUN, Lemma: text\n"
     ]
    }
   ],
   "source": [
    "# Tokenization Examples\n",
    "\n",
    "# 1. Simple word tokenization (split by spaces)\n",
    "simple_tokens = sample_text.split()\n",
    "print(\"ðŸ”¹ Simple Split by Spaces:\")\n",
    "print(f\"Tokens: {simple_tokens[:10]}...\")  # Show first 10\n",
    "print(f\"Total tokens: {len(simple_tokens)}\\n\")\n",
    "\n",
    "# 2. NLTK word tokenization (handles punctuation better)\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk_words = word_tokenize(sample_text)\n",
    "print(\"ðŸ”¹ NLTK Word Tokenization:\")\n",
    "print(f\"Tokens: {nltk_words[:15]}...\")  # Show first 15\n",
    "print(f\"Total tokens: {len(nltk_words)}\\n\")\n",
    "\n",
    "# 3. Sentence tokenization\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"ðŸ”¹ Sentence Tokenization:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence.strip()}\")\n",
    "\n",
    "# 4. spaCy tokenization (most advanced)\n",
    "if nlp is not None:\n",
    "    doc = nlp(sample_text)\n",
    "    spacy_tokens = [token.text for token in doc]\n",
    "    print(f\"\\nðŸ”¹ spaCy Tokenization:\")\n",
    "    print(f\"Tokens: {spacy_tokens[:15]}...\")\n",
    "    print(f\"Total tokens: {len(spacy_tokens)}\")\n",
    "    \n",
    "    # Show token details\n",
    "    print(\"\\nðŸ” Token Details (first 10):\")\n",
    "    for token in list(doc)[:10]:\n",
    "        print(f\"'{token.text}' -> POS: {token.pos_}, Lemma: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439e9f4",
   "metadata": {},
   "source": [
    "## 3. Stopword Removal\n",
    "\n",
    "**What are stopwords?**\n",
    "Stopwords are common words like \"the\", \"a\", \"an\", \"in\", \"on\", \"at\" that occur frequently but don't carry much semantic meaning for most NLP tasks. Removing them can help focus on the important words and reduce noise in your data.\n",
    "\n",
    "**When to remove stopwords:**\n",
    "- Text classification\n",
    "- Topic modeling\n",
    "- Information retrieval\n",
    "- Keyword extraction\n",
    "\n",
    "**When NOT to remove stopwords:**\n",
    "- Sentiment analysis (words like \"not\" are crucial)\n",
    "- Language modeling\n",
    "- Machine translation\n",
    "- Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb1f7088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T05:12:59.458344Z",
     "start_time": "2025-10-07T05:12:59.017798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The quick brown fox jumps over the lazy dog in the beautiful garden.\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. NLTK Stopword Removal:\n",
      "NLTK stopwords count: 198\n",
      "Original tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'in', 'the', 'beautiful', 'garden', '.']\n",
      "Filtered tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'beautiful', 'garden']\n",
      "\n",
      "2. spaCy Stopword Removal:\n",
      "spaCy filtered tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'beautiful', 'garden']\n",
      "\n",
      "3. Custom Stopword List:\n",
      "Custom stopwords: {'in', 'over', 'a', 'an', 'the'}\n",
      "Custom filtered tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'beautiful', 'garden']\n",
      "\n",
      "============================================================\n",
      "COMPARISON:\n",
      "Original word count: 14\n",
      "After NLTK filtering: 8\n",
      "After spaCy filtering: 8\n",
      "After custom filtering: 8\n"
     ]
    }
   ],
   "source": [
    "# Stopword Removal Examples\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Download NLTK stopwords if not already available\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample text for demonstration\n",
    "text = \"The quick brown fox jumps over the lazy dog in the beautiful garden.\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Method 1: Using NLTK\n",
    "print(\"\\n1. NLTK Stopword Removal:\")\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text.lower())\n",
    "\n",
    "filtered_nltk = [word for word in word_tokens if word not in stop_words_nltk and word.isalpha()]\n",
    "print(f\"NLTK stopwords count: {len(stop_words_nltk)}\")\n",
    "print(f\"Original tokens: {word_tokens}\")\n",
    "print(f\"Filtered tokens: {filtered_nltk}\")\n",
    "\n",
    "# Method 2: Using spaCy\n",
    "print(\"\\n2. spaCy Stopword Removal:\")\n",
    "doc = nlp(text.lower())\n",
    "filtered_spacy = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "print(f\"spaCy filtered tokens: {filtered_spacy}\")\n",
    "\n",
    "# Method 3: Custom stopwords\n",
    "print(\"\\n3. Custom Stopword List:\")\n",
    "custom_stopwords = {'the', 'in', 'over', 'a', 'an'}\n",
    "filtered_custom = [word for word in word_tokens if word.lower() not in custom_stopwords and word.isalpha()]\n",
    "print(f\"Custom stopwords: {custom_stopwords}\")\n",
    "print(f\"Custom filtered tokens: {filtered_custom}\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"Original word count: {len(word_tokens)}\")\n",
    "print(f\"After NLTK filtering: {len(filtered_nltk)}\")\n",
    "print(f\"After spaCy filtering: {len(filtered_spacy)}\")\n",
    "print(f\"After custom filtering: {len(filtered_custom)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2989f",
   "metadata": {},
   "source": [
    "## 4. Stemming and Lemmatization\n",
    "\n",
    "**What is Stemming?**\n",
    "Stemming is the process of reducing words to their root form by chopping off suffixes. It's a crude heuristic process that cuts off the ends of words.\n",
    "\n",
    "**What is Lemmatization?**\n",
    "Lemmatization is more sophisticated - it finds the dictionary form (lemma) of a word by considering the morphological analysis and part-of-speech.\n",
    "\n",
    "**Key Differences:**\n",
    "- **Stemming**: Fast, rule-based, may produce non-words (e.g., \"running\" â†’ \"run\", \"studies\" â†’ \"studi\")\n",
    "- **Lemmatization**: Slower, dictionary-based, produces real words (e.g., \"running\" â†’ \"run\", \"better\" â†’ \"good\")\n",
    "\n",
    "**When to use:**\n",
    "- **Stemming**: When speed is important and slight accuracy loss is acceptable\n",
    "- **Lemmatization**: When accuracy is crucial and you have computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ab57a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T05:13:25.256418Z",
     "start_time": "2025-10-07T05:13:22.342030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMING vs LEMMATIZATION COMPARISON\n",
      "============================================================\n",
      "Word         Stemming     NLTK Lemma   spaCy Lemma \n",
      "------------------------------------------------------------\n",
      "running      run          run          run         \n",
      "ran          ran          run          run         \n",
      "runs         run          run          run         \n",
      "easily       easili       easily       easily      \n",
      "fairly       fairli       fairly       fairly      \n",
      "studies      studi        study        study       \n",
      "studying     studi        study        study       \n",
      "studied      studi        study        study       \n",
      "better       better       better       well        \n",
      "good         good         good         good        \n",
      "children     children     child        child       \n",
      "feet         feet         foot         foot        \n",
      "teeth        teeth        teeth        tooth       \n",
      "geese        gees         goose        geese       \n",
      "mice         mice         mouse        mouse       \n",
      "\n",
      "============================================================\n",
      "PROCESSING A SENTENCE:\n",
      "Original: The children were running and studying different subjects better than before.\n",
      "Stemmed: the children were run and studi differ subject better than befor\n",
      "NLTK Lemmatized: the child be run and study different subject better than before\n",
      "spaCy Lemmatized: the child be run and study different subject well than before\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatization Examples\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Initialize tools\n",
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample words to demonstrate differences\n",
    "sample_words = [\n",
    "    \"running\", \"ran\", \"runs\", \"easily\", \"fairly\", \n",
    "    \"studies\", \"studying\", \"studied\", \"better\", \"good\",\n",
    "    \"children\", \"feet\", \"teeth\", \"geese\", \"mice\"\n",
    "]\n",
    "\n",
    "print(\"STEMMING vs LEMMATIZATION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Word':<12} {'Stemming':<12} {'NLTK Lemma':<12} {'spaCy Lemma':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for word in sample_words:\n",
    "    # Stemming with Porter Stemmer\n",
    "    stemmed = porter_stemmer.stem(word)\n",
    "    \n",
    "    # NLTK Lemmatization (requires POS tag for better results)\n",
    "    lemmatized_nltk = lemmatizer.lemmatize(word, pos='v')  # trying as verb first\n",
    "    if lemmatized_nltk == word:  # if no change, try as noun\n",
    "        lemmatized_nltk = lemmatizer.lemmatize(word, pos='n')\n",
    "    \n",
    "    # spaCy Lemmatization\n",
    "    doc = nlp(word)\n",
    "    lemmatized_spacy = doc[0].lemma_\n",
    "    \n",
    "    print(f\"{word:<12} {stemmed:<12} {lemmatized_nltk:<12} {lemmatized_spacy:<12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING A SENTENCE:\")\n",
    "\n",
    "sentence = \"The children were running and studying different subjects better than before.\"\n",
    "print(f\"Original: {sentence}\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "# Stem all words\n",
    "stemmed_words = [porter_stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "print(f\"Stemmed: {' '.join(stemmed_words)}\")\n",
    "\n",
    "# Lemmatize with NLTK\n",
    "lemmatized_nltk_words = [lemmatizer.lemmatize(token, pos='v') for token in tokens if token.isalpha()]\n",
    "lemmatized_nltk_words = [lemmatizer.lemmatize(token, pos='n') if token == lemmatizer.lemmatize(token, pos='v') else lemmatizer.lemmatize(token, pos='v') for token in tokens if token.isalpha()]\n",
    "print(f\"NLTK Lemmatized: {' '.join(lemmatized_nltk_words)}\")\n",
    "\n",
    "# Lemmatize with spaCy\n",
    "doc = nlp(sentence.lower())\n",
    "lemmatized_spacy_words = [token.lemma_ for token in doc if token.is_alpha]\n",
    "print(f\"spaCy Lemmatized: {' '.join(lemmatized_spacy_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d1296",
   "metadata": {},
   "source": [
    "## 5. Text Cleaning and Normalization\n",
    "\n",
    "**What is Text Cleaning?**\n",
    "Text cleaning involves removing or standardizing unwanted characters, formatting, and noise from text data.\n",
    "\n",
    "**Common cleaning operations:**\n",
    "- Remove special characters, punctuation\n",
    "- Handle numbers and digits\n",
    "- Convert to lowercase\n",
    "- Remove extra whitespace\n",
    "- Handle contractions (don't â†’ do not)\n",
    "- Remove HTML tags, URLs, mentions\n",
    "- Normalize unicode characters\n",
    "\n",
    "**Why is it important?**\n",
    "- Reduces noise in the data\n",
    "- Ensures consistency\n",
    "- Improves model performance\n",
    "- Handles real-world messy text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a46942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T05:13:36.454214Z",
     "start_time": "2025-10-07T05:13:36.442929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT CLEANING DEMONSTRATION\n",
      "============================================================\n",
      "Original: \n",
      "Hey @user123! Check this out: https://example.com/article?id=123 \n",
      "It's AMAZING!!! Don't you think so??? I can't believe it's 2024! \n",
      "#NLP #TextProcessing    #DataScience\n",
      "\n",
      "Lowercase: \n",
      "hey @user123! check this out: https://example.com/article?id=123 \n",
      "it's amazing!!! don't you think so??? i can't believe it's 2024! \n",
      "#nlp #textprocessing    #datascience\n",
      "\n",
      "Contractions expanded: \n",
      "hey @user123! check this out: https://example.com/article?id=123 \n",
      "it's amazing!!! do not you think so??? i cannot believe it's 2024! \n",
      "#nlp #textprocessing    #datascience\n",
      "\n",
      "URLs removed: \n",
      "hey @user123! check this out:  \n",
      "it's amazing!!! do not you think so??? i cannot believe it's 2024! \n",
      "#nlp #textprocessing    #datascience\n",
      "\n",
      "Mentions/hashtags removed: \n",
      "hey ! check this out:  \n",
      "it's amazing!!! do not you think so??? i cannot believe it's 2024! \n",
      "     \n",
      "\n",
      "Numbers removed: \n",
      "hey ! check this out:  \n",
      "it's amazing!!! do not you think so??? i cannot believe it's ! \n",
      "     \n",
      "\n",
      "Punctuation removed: \n",
      "hey  check this out  \n",
      "its amazing do not you think so i cannot believe its  \n",
      "     \n",
      "\n",
      "Extra whitespace removed: hey check this out its amazing do not you think so i cannot believe its\n",
      "\n",
      "============================================================\n",
      "ADDITIONAL CLEANING TECHNIQUES\n",
      "============================================================\n",
      "HTML tags removed: <p>This is a <strong>HTML</strong> text with <a href='#'>links</a>.</p> â†’ This is a HTML text with links.\n",
      "Unicode normalized: CafÃ© naÃ¯ve rÃ©sumÃ© â†’ Cafe naive resume\n",
      "Special chars removed: Remove these: !@#$%^&*()_+ â†’ Remove these _\n",
      "Single chars removed: This is a b c d example text â†’ This is example text\n"
     ]
    }
   ],
   "source": [
    "# Text Cleaning and Normalization Examples\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \"\"\"\n",
    "    # Original text\n",
    "    print(f\"Original: {text}\")\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    print(f\"Lowercase: {text}\")\n",
    "    \n",
    "    # 2. Handle contractions\n",
    "    contractions = {\n",
    "        \"don't\": \"do not\",\n",
    "        \"won't\": \"will not\", \n",
    "        \"can't\": \"cannot\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    print(f\"Contractions expanded: {text}\")\n",
    "    \n",
    "    # 3. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    print(f\"URLs removed: {text}\")\n",
    "    \n",
    "    # 4. Remove mentions and hashtags (social media)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    print(f\"Mentions/hashtags removed: {text}\")\n",
    "    \n",
    "    # 5. Remove numbers (optional - depends on use case)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    print(f\"Numbers removed: {text}\")\n",
    "    \n",
    "    # 6. Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    print(f\"Punctuation removed: {text}\")\n",
    "    \n",
    "    # 7. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    print(f\"Extra whitespace removed: {text}\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test with messy text\n",
    "messy_text = \"\"\"\n",
    "Hey @user123! Check this out: https://example.com/article?id=123 \n",
    "It's AMAZING!!! Don't you think so??? I can't believe it's 2024! \n",
    "#NLP #TextProcessing    #DataScience\n",
    "\"\"\"\n",
    "\n",
    "print(\"TEXT CLEANING DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "cleaned = clean_text(messy_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADDITIONAL CLEANING TECHNIQUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# HTML tag removal example\n",
    "html_text = \"<p>This is a <strong>HTML</strong> text with <a href='#'>links</a>.</p>\"\n",
    "clean_html = re.sub(r'<[^>]+>', '', html_text)\n",
    "print(f\"HTML tags removed: {html_text} â†’ {clean_html}\")\n",
    "\n",
    "# Unicode normalization example\n",
    "unicode_text = \"CafÃ© naÃ¯ve rÃ©sumÃ©\"\n",
    "normalized = unicodedata.normalize('NFKD', unicode_text).encode('ascii', 'ignore').decode('ascii')\n",
    "print(f\"Unicode normalized: {unicode_text} â†’ {normalized}\")\n",
    "\n",
    "# Custom character removal\n",
    "special_chars = \"Remove these: !@#$%^&*()_+\"\n",
    "clean_special = re.sub(r'[^\\w\\s]', '', special_chars)\n",
    "print(f\"Special chars removed: {special_chars} â†’ {clean_special}\")\n",
    "\n",
    "# Remove single characters (often noise after cleaning)\n",
    "single_char_text = \"This is a b c d example text\"\n",
    "no_single_chars = ' '.join([word for word in single_char_text.split() if len(word) > 1])\n",
    "print(f\"Single chars removed: {single_char_text} â†’ {no_single_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d69a7a",
   "metadata": {},
   "source": [
    "## 6. Complete Preprocessing Pipeline\n",
    "\n",
    "**Putting it all together:**\n",
    "Now we'll create a comprehensive preprocessing pipeline that combines all the techniques we've learned. This pipeline will be modular, allowing you to enable/disable specific preprocessing steps based on your needs.\n",
    "\n",
    "**Pipeline steps:**\n",
    "1. Text cleaning and normalization\n",
    "2. Tokenization\n",
    "3. Stopword removal\n",
    "4. Stemming or Lemmatization\n",
    "5. Filtering (remove short words, numbers, etc.)\n",
    "\n",
    "**Flexibility:**\n",
    "You can customize the pipeline based on your specific NLP task and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2febef1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T05:14:11.454306Z",
     "start_time": "2025-10-07T05:14:10.721132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING PIPELINE DEMONSTRATION\n",
      "================================================================================\n",
      "\n",
      "1. FULL PREPROCESSING (lemmatization, stopword removal):\n",
      "Text 1:\n",
      "  Original: The quick brown fox jumps over the lazy dog!\n",
      "  Processed: quick brown fox jump lazy dog\n",
      "Text 2:\n",
      "  Original: I can't believe it's 2024! This is AMAZING!!! Don't you think so?\n",
      "  Processed: believe amazing think\n",
      "Text 3:\n",
      "  Original: Check out https://example.com for more information @user123 #NLP\n",
      "  Processed: check information\n",
      "Text 4:\n",
      "  Original: The children were running and studying better than before.\n",
      "  Processed: child run study well\n",
      "\n",
      "2. MINIMAL PREPROCESSING (stemming, keep stopwords):\n",
      "Text 1:\n",
      "  Original: The quick brown fox jumps over the lazy dog!\n",
      "  Processed: the quick brown fox jump over the lazi dog\n",
      "Text 2:\n",
      "  Original: I can't believe it's 2024! This is AMAZING!!! Don't you think so?\n",
      "  Processed: can not believ it thi is amaz do not you think so\n",
      "\n",
      "3. BATCH PROCESSING EXAMPLE:\n",
      "Batch processed texts:\n",
      "  'Natural Language Processing is fascinating!' â†’ 'natural language processing fascinate'\n",
      "  'Machine learning models require clean data.' â†’ 'machine learning model require clean datum'\n",
      "  'Text preprocessing is a crucial step.' â†’ 'text preprocesse crucial step'\n"
     ]
    }
   ],
   "source": [
    "# Complete Text Preprocessing Pipeline\n",
    "# Initialize tools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK data\n",
    "import nltk\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A comprehensive text preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 remove_stopwords=True,\n",
    "                 use_lemmatization=True,  # If False, will use stemming\n",
    "                 remove_punctuation=True,\n",
    "                 to_lowercase=True,\n",
    "                 remove_numbers=True,\n",
    "                 min_word_length=2):\n",
    "        \n",
    "\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        \n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.to_lowercase = to_lowercase\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.min_word_length = min_word_length\n",
    "        \n",
    "        # Initialize NLTK tools\n",
    "        if self.remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        if self.use_lemmatization:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "        else:\n",
    "            self.stemmer = PorterStemmer()\n",
    "            \n",
    "        # Try to load spaCy model (fallback to NLTK if not available)\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "            self.use_spacy = True\n",
    "        except:\n",
    "            self.use_spacy = False\n",
    "            print(\"spaCy model not found, using NLTK for tokenization\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        import re\n",
    "        import string\n",
    "        \n",
    "        if self.to_lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Handle contractions\n",
    "        contractions = {\n",
    "            \"don't\": \"do not\", \"won't\": \"will not\", \"can't\": \"cannot\",\n",
    "            \"n't\": \" not\", \"'re\": \" are\", \"'ve\": \" have\", \n",
    "            \"'ll\": \" will\", \"'d\": \" would\", \"'m\": \" am\"\n",
    "        }\n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        \n",
    "        # Remove URLs, mentions, hashtags\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        \n",
    "        # Remove numbers if specified\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove punctuation if specified\n",
    "        if self.remove_punctuation:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Clean extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text using spaCy or NLTK\"\"\"\n",
    "        if self.use_spacy:\n",
    "            doc = self.nlp(text)\n",
    "            return [token.text for token in doc if token.is_alpha and len(token.text) >= self.min_word_length]\n",
    "        else:\n",
    "            from nltk.tokenize import word_tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "            return [token for token in tokens if token.isalpha() and len(token) >= self.min_word_length]\n",
    "    \n",
    "    def remove_stopwords_func(self, tokens):\n",
    "        \"\"\"Remove stopwords from token list\"\"\"\n",
    "        if not self.remove_stopwords:\n",
    "            return tokens\n",
    "        return [token for token in tokens if token.lower() not in self.stop_words]\n",
    "    \n",
    "    def normalize_tokens(self, tokens):\n",
    "        \"\"\"Apply stemming or lemmatization\"\"\"\n",
    "        if self.use_lemmatization:\n",
    "            if self.use_spacy:\n",
    "                # Use spaCy for lemmatization\n",
    "                doc = self.nlp(' '.join(tokens))\n",
    "                return [token.lemma_ for token in doc]\n",
    "            else:\n",
    "                # Use NLTK lemmatizer\n",
    "                return [self.lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "        else:\n",
    "            # Use stemming\n",
    "            return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    def preprocess(self, text, return_string=True):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to preprocess\n",
    "            return_string: If True, returns joined string; if False, returns list of tokens\n",
    "        \n",
    "        Returns:\n",
    "            Preprocessed text as string or list of tokens\n",
    "        \"\"\"\n",
    "        # Step 1: Clean text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Step 2: Tokenize\n",
    "        tokens = self.tokenize(cleaned_text)\n",
    "        \n",
    "        # Step 3: Remove stopwords\n",
    "        tokens = self.remove_stopwords_func(tokens)\n",
    "        \n",
    "        # Step 4: Normalize (stem or lemmatize)\n",
    "        tokens = self.normalize_tokens(tokens)\n",
    "        \n",
    "        if return_string:\n",
    "            return ' '.join(tokens)\n",
    "        else:\n",
    "            return tokens\n",
    "\n",
    "# Test the pipeline with different configurations\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog!\",\n",
    "    \"I can't believe it's 2024! This is AMAZING!!! Don't you think so?\",\n",
    "    \"Check out https://example.com for more information @user123 #NLP\",\n",
    "    \"The children were running and studying better than before.\"\n",
    "]\n",
    "\n",
    "print(\"PREPROCESSING PIPELINE DEMONSTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configuration 1: Full preprocessing\n",
    "print(\"\\n1. FULL PREPROCESSING (lemmatization, stopword removal):\")\n",
    "preprocessor1 = TextPreprocessor(\n",
    "    remove_stopwords=True,\n",
    "    use_lemmatization=True,\n",
    "    remove_punctuation=True,\n",
    "    to_lowercase=True,\n",
    "    remove_numbers=True\n",
    ")\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    result = preprocessor1.preprocess(text)\n",
    "    print(f\"Text {i}:\")\n",
    "    print(f\"  Original: {text}\")\n",
    "    print(f\"  Processed: {result}\")\n",
    "\n",
    "# Configuration 2: Minimal preprocessing\n",
    "print(\"\\n2. MINIMAL PREPROCESSING (stemming, keep stopwords):\")\n",
    "preprocessor2 = TextPreprocessor(\n",
    "    remove_stopwords=False,\n",
    "    use_lemmatization=False,\n",
    "    remove_punctuation=True,\n",
    "    to_lowercase=True,\n",
    "    remove_numbers=False\n",
    ")\n",
    "\n",
    "for i, text in enumerate(sample_texts[:2], 1):  # Just first 2 texts for brevity\n",
    "    result = preprocessor2.preprocess(text)\n",
    "    print(f\"Text {i}:\")\n",
    "    print(f\"  Original: {text}\")\n",
    "    print(f\"  Processed: {result}\")\n",
    "\n",
    "# Batch processing example\n",
    "print(\"\\n3. BATCH PROCESSING EXAMPLE:\")\n",
    "all_texts = [\n",
    "    \"Natural Language Processing is fascinating!\",\n",
    "    \"Machine learning models require clean data.\",\n",
    "    \"Text preprocessing is a crucial step.\",\n",
    "]\n",
    "\n",
    "batch_results = [preprocessor1.preprocess(text) for text in all_texts]\n",
    "print(\"Batch processed texts:\")\n",
    "for original, processed in zip(all_texts, batch_results):\n",
    "    print(f\"  '{original}' â†’ '{processed}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77c637",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways and Next Steps\n",
    "\n",
    "### What we covered:\n",
    "1. **Tokenization**: Breaking text into individual words or tokens\n",
    "2. **Stopword Removal**: Filtering out common, low-information words\n",
    "3. **Stemming vs Lemmatization**: Reducing words to their base forms\n",
    "4. **Text Cleaning**: Removing noise, special characters, and standardizing format\n",
    "5. **Complete Pipeline**: Combining all techniques in a modular, configurable way\n",
    "\n",
    "### When to use each technique:\n",
    "- **Always use**: Text cleaning and tokenization\n",
    "- **Use for most tasks**: Stopword removal, lowercasing\n",
    "- **Choose based on task**: \n",
    "  - Stemming for speed (information retrieval)\n",
    "  - Lemmatization for accuracy (sentiment analysis, classification)\n",
    "- **Task-specific**: Number removal, punctuation handling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
